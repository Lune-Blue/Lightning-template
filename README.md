# âš¡ PyTorch Lightning Training Template

A modular and extensible deep learning training template built with [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/).  
This project is designed to support flexible and scalable training of transformer-based models with built-in support for:

- âœ… HuggingFace Transformers
- âœ… PEFT / LoRA Fine-tuning
- âœ… Accelerate-based training
- âœ… Custom training configs via YAML
- âœ… W&B logging
- âœ… Learning rate finder
- âœ… Continual training & testing

---

## ðŸš€ Project Structure

```bash
.
â”œâ”€â”€ main.py                    # Main training entry point
â”œâ”€â”€ config/                   # YAML config files
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ BackboneModel.py      # Lightning model definition
â”‚   â””â”€â”€ trainer.py            # LightningModule wrapper
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataclass.py          # Argument schema definitions
â”‚   â”œâ”€â”€ dataset.py            # DataModule definition
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ utility.py            # Helpers (e.g., accelerate model, checkpointing)

ðŸ§© Config-based training
Easily customize all arguments (model, data, training, LoRA, generation, etc.) using a YAML config.

âš¡ Lightning Trainer with Callbacks
Integrated with ModelCheckpoint, EarlyStopping, LearningRateMonitor, and custom LR finder.

ðŸ“š LoRA / Adapter Support
Lightweight fine-tuning with PEFT for large language models.

ðŸ§ª Testing Mode
Automatically loads best checkpoints and evaluates model.

ðŸ“Š Weights & Biases Logging
Full integration with wandb for tracking metrics and artifacts.