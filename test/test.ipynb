{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"alespalla/chatbot_instruction_prompts\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_X, Valid_X, Train_Y, Valid_Y = train_test_split(dataset['train']['response'], dataset['train']['prompt'], test_size = 0.1, random_state=42)\n",
    "print(len(Train_X))\n",
    "print(len(Valid_X))\n",
    "print(len(Train_Y))\n",
    "print(len(Valid_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 2,3\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', truncation_side='right', hi=5)\n",
    "example = \"my name is Sylvain and i work at hugging face\"\n",
    "encoding = tokenizer(example, return_tensors='pt')\n",
    "print(encoding)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', truncation_side='left')\n",
    "other_encoding = tokenizer(example, max_length=10)\n",
    "print(other_encoding)\n",
    "print(len(other_encoding['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-cased')\n",
    "output = model(**encoding)\n",
    "\n",
    "output2 = model(encoding)\n",
    "\n",
    "assert output == output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Accept': 'text/plain',\n",
    "    'Content_Length': 348,\n",
    "    'Host': 'http://mingrammer.com'\n",
    "}\n",
    "\n",
    "def pre_process(**headers):\n",
    "    content_length = headers['Content_Length']\n",
    "    print('content length: ', content_length)\n",
    "\n",
    "    print(headers)\n",
    "\n",
    "pre_process(**headers)\n",
    "pre_process(Accept = 'text/plain', Content_Length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "class M(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        print(kwargs)\n",
    "        \n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config', type=str, default='src/frozen/config/train.yaml')\n",
    "    parser.add_argument('--config_name', type=str, default='base')\n",
    "    parser.add_argument('--percent', type=float, default=-1)\n",
    "    parser.add_argument('--only_vision', action='store_true', help='using only vision model')\n",
    "    parser.add_argument('--without_rationale', action='store_true', help='using only without rationale')\n",
    "    parser.add_argument('--answerlossonly', action='store_true', help='using onlyanswerloss only')\n",
    "    parser.add_argument('--pretrained_path', type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "args = get_args()\n",
    "make_object = M(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2', truncation_side='right', hi=5)\n",
    "# example = \"my name is Sylvain and i work at hugging face\"\n",
    "# encoding = tokenizer(example, return_tensors = 'pt')\n",
    "# tokenizer.pad(encoding, True, return_tensors = 'pt')\n",
    "# print(encoding)\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "# output = model.generate(**encoding)\n",
    "# print(output)\n",
    "# second_output = model(**encoding)\n",
    "# print(second_output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 29631,     5,  1219,     9, 31832,     1],\n",
      "        [    2, 11613,   857,    16,   110, 21039,   116]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([2, 7])\n",
      "torch.Size([2, 7, 50272])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from torch import nn\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b', truncation_side='right')\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "make_input = [\"write the reason of raining\", \"whay is your hobby?\"]\n",
    "make_output = [\"i don't love you\", \"my name is yujin.\"]\n",
    "input_encoding = tokenizer(make_input)\n",
    "output_encoding = tokenizer(make_output)\n",
    "\n",
    "padded_encoding = tokenizer.pad(input_encoding, True, return_tensors = 'pt')\n",
    "print(padded_encoding)\n",
    "print(padded_encoding['input_ids'].size())\n",
    "output = model(**padded_encoding)\n",
    "print(output.logits.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n",
      "tensor([[  100,    10,  3260,    13,     5,    11,  1437,   100],\n",
      "        [  100,  2923,    16,    42,  2674,   116, 50118,   116]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Iats is this favorite?\\n?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "b = torch.argmax(output.logits, dim=2)\n",
    "print(b.size())\n",
    "print(b)\n",
    "tokenizer.decode(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  4783,   766,    16, 28856,  1851,     8,   939,   173,    23,\n",
      "         31164,   652,   939,   657,    47],\n",
      "        [    2,  4783,   766,    16,   842,  1545, 18115, 50265, 50265, 50265,\n",
      "         50265, 50265, 50265, 50265, 50265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': tensor([[    2,  4783,   766,    16, 28856,  1851,     8,   939,   173,    23,\n",
      "         31164,   652,   939,   657,    47],\n",
      "        [    2,  4783,   766,    16,   842,  1545, 18115, 50265, 50265, 50265,\n",
      "         50265, 50265, 50265, 50265, 50265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "acca\n"
     ]
    }
   ],
   "source": [
    "a = {k:v for k,v in padded_encoding.items()}\n",
    "print(a)\n",
    "print(padded_encoding)\n",
    "if a==padded_encoding:\n",
    "    print('acca')\n",
    "else:\n",
    "    print('qqq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17, 50266])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lune/.conda/envs/lune-blue/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 29631,     5,  1219,     9, 31832,  1437,     2,   100,   437,\n",
      "            45,   686,   114,    42,    16,    10,   205,  1114,     4,  1437],\n",
      "        [    2, 11613,   857,    16,   110, 21039,   116, 50265,   116, 50118,\n",
      "           100,   437,    10,  9613,     4,    38,   310,  8669,     8,  9590]])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**padded_encoding)\n",
    "print(output)\n",
    "tokenizer.decode(output[0])\n",
    "print(output[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [[1,2,3],[4,5,6]]\n",
    "b = np.array(a)\n",
    "print(b[:,1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/convei_nas2/lune/lightning/lightning-template', '/home/lune/.conda/envs/lune-blue/lib/python39.zip', '/home/lune/.conda/envs/lune-blue/lib/python3.9', '/home/lune/.conda/envs/lune-blue/lib/python3.9/lib-dynload', '', '/home/lune/.conda/envs/lune-blue/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<pad>\n",
      "{'input_ids': [2, 10975, 19320, 742], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b', truncation_side='right')\n",
    "print(tokenizer._pad_token)\n",
    "print(tokenizer._pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lune-blue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
